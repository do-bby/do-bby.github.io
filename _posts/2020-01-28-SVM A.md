---
title: "SVM알고리즘"
categories: 
  - blogging
last_modified_at: 2020-01-29T15:00:00+09:00
toc: true
---

##SVM(Support Vector Machine)##

서포트 벡터 머신은 딥 러닝이 나온 이후에도 여전히 사용되고 있는 머신러닝 알고리즘입니다.
딥러닝 못지 않은 성능을 내고 무엇보다 가볍기 때문입니다.
한마디로 SVM을 말하자면 데이터를 선형으로 분리하는 최적의 선형 결정 경계를 찾는 알고리즘입니다.

SVM은 RBF(radial basis function)커널을 사용한 SVM인데,
거의 모든 머신러닝 알고리즘이 그렇듯 RBF커널 SVM도 두개의 매개변수(C, gamma)가 사용자에 의해
세팅 되어야합니다.

이 두개의 매개변수를 grid search라는 경험적인 방법의 의해서만 선택합니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/GS.png" alt=""> {% endraw %}

gridsearch란 그림과 같이 매개변수들의 여러 조합들을 테스트해서 가장 좋은 성능을 내는
매개변수를 찾아내는 것입니다.
매개변수들이 SVM에서 어떤 역할을 하는지 의미를 알면,
좀 더 빠르게 적합한 매개변수값들을 찾아 낼 수 있습니다.

RBF커널 SVM을 알기 위해서 먼저 선형 SVM에 대해 알아야하는데,
선형 SVM에서 사용자가 설정해야 하는 매개변수는 Cost이고, RBF커널 SVM에서는 Cost와 함께 gamma라는 매개변수를
조정해야합니다.

##선형SVM##

SVM에서 가장 간단한 것이 선형 SVM인데,
SVM알고리즘의 목표는 그림과 같이 클래스가 다른 데이터들을 가장 큰 마진으로 분리해내는 선 또는 면을
찾는것입니다. 여기서 이 선 또는 면을 결정 경계라고 부릅니다.
마진이란 두 데이터 군과 결정 경계와 떨어져있는 정도를 의미합니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/LS.png" alt=""> {% endraw %}

그림에서 H1의 경우 데이터를 제대로 분류하지못햇고,
H2는 분류하기는 했지만 작은 마진으로 분류했습니다.
H3이 가장 큰 마진으로 데이터를 적절히 분류해낸 결정 경계가 되는데,
H2는 데이터들과 가까운 반면, H3은 데이터들과 멀리 떨어져 있습니다.
결과적으로 이후에 새로운 데이터가 추가되더라도 H3이 더 안정적으로 데이터를 분류 할 수 있습니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/LS2.png" alt=""> {% endraw %}

그림에서와 같이 4개의 데이터가 추가된 상황에서 H3과 달리 H2는 데이터를
제대로 분류 해낼 수 없습니다.
이러한 이유로 아까도 말햇다시피 SVM의 목표는 마진이 가장 큰 결정 경계를 찾는 것 입니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/M.png" alt=""> {% endraw %}

서포트 벡터 머신에서 서포트 벡터란 두 클래스 사이의 경계에 위치한 데이터 포인터들입니다.
많은 데이터가 있지만 서포트 벡터는 결정 경계를 만드는데 영향을 줍니다.
이 데이터들의 위치에 따라 결정 경계의 위치가 달리집니다.
즉 이 데이터들이 결정 경계를 지지(Support)하고 있다고 말할 수 있기 때문에,
서포트벡터라고 부르는것입니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/O.png" alt=""> {% endraw %}

그러나 대부분의 데이터는 이상적으로 분리되어 있지 않은 경우가 많은데,
그림과 같은 경우 데이터들을 선형적으로 완벽하게 분리하기 불가능한 경우가 있습니다.
이것을 해결하기 위해 오류를 허용하는 전략이 있는데 이와 관련된 매개변수가 Cost입니다.
Cost는 얼마나 많은 데이터 샘플이 다른 클래스에 놓이는 것을 허용하는지를 결정합니다.
작을 수록 많이 허용하고, 클 수록 적게 허용합니다.
예를 들어 데이터 샘플하나도 잘못 분류할 수 없을 땐 Cost를 높여야합니다.
반대로 몇개 정도는 놓쳐도 괜찮을 땐 Cost를 낮추면 됩니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/Cost.png" alt=""> {% endraw %}

왼쪽과 같이 Cost가 낮을 때는 하나의 데이터를 잘못 분류했지만 좀더 일반적인 결정 경계를 찾았습니다.
반면 오른쪽과 같이 Cost가 높을 때는 실수 없이 분류했지만, 
새로운 데이터가 어느 클래스에 속하는지 예측할 땐 좋은 성능을 낼 수 없을 것입니다.
Cost가 너무 낮으면 과소적합(underfitting)이 될 가능성이 높아지고,
Cost가 너무 높으면 과대적합(overfitting)이 될 가능성이 높아지므로 적합한 Cost값을 찾는것이 중요합니다.

##RBF 커널 SVM##

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/RBF.png" alt=""> {% endraw %}

위와 같이 선형 SVM으로는 제대로 분류할 수 없는 상황이 있을 땐,
커널 기법을 사용하면 되는데
커널기법이란 주어진 데이터를 고차원 특징 공간으로 사상해주는것 입니다.
고차원 공간에 사상되면 원래의 차원에서는 보이지 않던 선형으로 분류해줄 수 있는 방법이 보입니다.


{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/kernel.png" alt=""> {% endraw %}

2차원공간에서는 선형적으로 분류할 수 없던 데이터 분포가 커널기법을 사용해 3차원공간으로 사상되니
분류가 가능해졌습니다.

커널에는 Polynomial 커널, Sigmoid 커널, 가우시안 RBF 커널 등 종류가 많은데,
그 중 성능이 가장 좋아 사용되고 있는 것이 가우시안 RBF 커널입니다.
SVM에서 기본 매개변수인 Cost와 RBF커널에서 사용되는 매개변수는 gamma라는 매개변수
총 2개의 매개변수를 설정해줘야합니다.
gamma는 하나의 데이터 샘플이 영향력을 행사하는 거리를 결정합니다.
gamma가 클수록 데이터 포인터들이 영향력을 행사하는 거리가 짧아진다면,
반대로 낮을수록 커집니다.

{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/g.png" alt=""> {% endraw %}


{% raw %} <img src="https://qkrdbstn15.github.io/assets/img/cg.png" alt=""> {% endraw %}

Cost가 커지면 이상치의 존재 가능성을 낮게 본다고 말했는데,
gamma=10인 열을 보면 ,
Cost =1 일때 두개의 이상치를 인정하고 결정 경계를 찾았지만,
Cost =100일때 한개의 이상치를 인정했지만 Cost=10일때보다 살짝 부자연스럽게 분류해냈습니다.


Cost=1인 행을 보면,
왼쪽에서 오른쪽으로 갈수록 gamma가 더 커지는데,
결정 경계가 결정 경계 가까이에 있는 데이터 샘플들에 영향을 크게 받기 때문에 
구불구불해지는 것을 알 수 있습니다.
즉 gamma는 결정 경계의 곡률을 조정한다고 말 할 수 있습니다.
gamma가 높아질수록 파란색 공간이 작아지는데,
gamma가 높아질수록 데이터 포인터가 영향력을 행사하는 거리가 짧아졌기 때문입니다.





 




